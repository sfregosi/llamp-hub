{
  "articles": [
    {
      "path": "analysis.html",
      "title": "Protocol - Analysis",
      "description": "Detailed instructions for post-processing and acoustic analyses of longline HARP data\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPost-processing\r\nCreate GitHub issue\r\nMetadata and data checks\r\nProcess trip\r\nFinal checks\r\n\r\nAcoustic analyses\r\nCreate GitHub issue\r\nOdontocetes\r\nEvent annotation\r\nLogging in Triton\r\n\r\nAutomated detection\r\nPamguard setup\r\nRun Pamguard\r\nBackup outputs\r\n\r\nClassification\r\nCloning code\r\nRunning classification\r\n\r\n\r\nNoise\r\nUnderwater recordings\r\n\r\nIn-air recordings\r\n\r\nPast protocols\r\nLast updated\r\n\r\n\r\n\r\n\r\n\r\nPost-processing\r\nFirst there is a set of tasks that are sort of post-processing/pre-analysis - where data are checked and metadata updated.\r\nCreate GitHub issue\r\nCreate a new post-processing issue for the latest trip\r\nNavigate to the llamp repository’s New Issue page and press the green Get Started button for the Post-processing template\r\nUpdate the newly created issue name with the correct trip number, add any assignees, and add to the Longline Acoustic Monitoring Project\r\n\r\n\r\n\r\n\r\nFigure 1: Screenshot showing the new issue template selection page on GitHub\r\n\r\n\r\n\r\nMetadata and data checks\r\nAdd trip to phase_3_multi_deployments_summary.xlsx on 5 tabs\r\nDataSummary_UW: add 5 rows following format of previous trips with trip number in column A, vessel name and frame/datalogger numbers in column B, and the date for each deployed set (1 up to 8) in columns C to J). Note any problems or inconsistencies for each recorder in the ‘notes’ column\r\nDataSummary_IA: added 7 rows, copying the ‘Status’ and ‘Location’ column values from an earlier trip\r\nDepredation: provide a summary of the trip dates and Y or N for depredation with notes on specifically which sets, if any, had bait or catch depredation\r\nProcessingStatus: add a row for each recorder deployed and use this track steps as they are completed for each recorder + any notes on issues\r\nAnalysisStatus: add a row for each recorder and track analysis steps (below) here\r\nBouts: not used for now. Current data is from Phase I and II analyses and is just left there as a placeholder\r\n\r\nCreate README_LL0##.txt file on server (//PICQUEENFISH/PSD2/CRP/LLHARP) within the folder for this trip. This README will house any notes about data issues/trip inconsistencies while working through post-processing and analysis steps\r\nCheck observer datasheets for readability and combine if needed (if photographed rather than scanned)\r\nIn Adobe Acrobat Pro: File > Create > Combine Files into a Single PDF…\r\nDrag or select all photo files\r\nUse the Scan & OCR tools to ‘Enhance’ each page\r\nAfter selecting Scan & OCR on the right hand tools menu, click Enhance > Camera Image from the top tools menu\r\nPage borders will appear. Adjust the extent of the page borders to include the whole page and exclude any background and click Enhance Page\r\nThe default enhance level usually obscures some of the entries so reduce the enhancement level using the Adjust enhancement level slider until it is readable\r\nClick on the next thumbnail to repeat for the next page\r\n\r\nSave within the trip’s server folder with the filename LL0##_observer_sheets_photos.pdf\r\n\r\nTranscribe underwater and in-air datasheets to .xlsx\r\nUse the blank template found on SF’s local machine at C:\\Users\\Selene.Fregosi\\Documents\\longline_local\\tripExcelSummaries\\\r\nLL000_Summary_phase3_template.xlsx or in the llamp repository at LL000_Summary_phase3template.xlsx\r\nSave the template as LL0##_Summary.xlsx within this trip’s server folder\r\nFill in observer noted set/haul/recorder deployment times in the Summary tab\r\nFill in observer noted in-air recording times and filenames in the In-air tab\r\nUpdated the 4 RecorderTimes_FR## tabs with the correct frame numbers\r\n\r\nReview underwater data LTSA in Triton\r\nMark actual recorder in/out times from the acoustics and record the in/out times in the RecorderTimes tab(s) of LL0##_summary.xlsx created above\r\nUse one tab per recorder/LTSA\r\nRecordings are in UTC, so note in/out times in UTC and subtract 10 hours to also convert to local time for comparison to observer noted times\r\n\r\nNote data availability and quality in the DataSummary_UW tab of phase_3_multi_deployments_summary.xlsx\r\nFill green for audio, red for no audio\r\nNote the recorder deployment order (1-4) for each set\r\nMake comments for any irregularities found such as noise, on deck recordings (recorder did not automatically turn off after recovery), data gaps, etc.\r\nReport any problems with noise or missing data to the LLHARP hardware manager (likely Erik Norris)\r\n\r\n\r\nReview in-air data in Raven (or other?)\r\nLoad all files for a single recorder (e.g. Z03 or Z04) by selecting all files and dragging into the Raven window\r\nSelect Page files and set page length to 120 or 180 seconds\r\nEach file is separated by a green dashed line; can move through files with black arrow at top right or by selecting the file number from the dropdown\r\n\r\nConfirm file numbers, durations, and voice comments align with what is on the datasheet and transcribed to L0##_summary.xlsx\r\nNote data availability and quality in the DataSummary_IA tab of phase_3_multi_deployments_summary.xlsx\r\nFill green for audio, red for no audio\r\nSpecify the file number for each set and recording sample\r\nNote any irregularities such as missing data or incorrect timing\r\n\r\n\r\nBack to top\r\nProcess trip\r\nRun workflow_processTrip.m to work through converting the summary docs to .mat, running some log checks, and splitting up .xwavs that span multiple sets.\r\nSet trip information in top section (unnumbered) of workflow_processTrip.m\r\nTrip number trip = on line 41\r\nFrames to process (default is frToProcess = 'all'; on line 43\r\nPaths on lines 46 to 62\r\n\r\nRun section (1) to convert LL0##_Summary.xlsx times and locations to a .mat file\r\nCheck for timing inconsistencies as prompted\r\nCheck any location inconsistencies from the generated plot\r\nNote any issues and try to fix discrepancies where possible, noting these issues or corrections in the README\r\n\r\nRun section (2) to manually enter the frame and datalogger numbers when prompted\r\nThe format should be FR##_DL## (e.g., FR04_DL64)\r\nHit Enter after each entry, and Enter again after all are entered to exit input state\r\n\r\nRun section (3) to summarize the recording durations, from the observer noted times and the acoustic record\r\nManually check durations for any large discrepancies between the acoustic record and observer log\r\nIf corrections are needed, make these corrections and note them in the README\r\n\r\nRun section (4) to work through the xwavs that span across sets and split the files as needed to preserve recording timing information for later analysis\r\nThis section will pause after calculating the number of file gaps. Manually compare this number to the observations in the manually marked recorder in/out times and if they match, hit Enter to continue\r\nThis section will take some time and will save ‘original’ xwavs before writing new, split xwavs\r\nUpdate the README with info on the number of original xwavs, gaps, and deck test or on deck recordings and any other relevent notes\r\n\r\nOptional, run section (5) to plot the drifts again and TDR data, for reporting purposes. Section 5 is under development\r\nBack to top\r\nFinal checks\r\nAfter workflow_processTrip.m, finalize the created ltsas and summary docs.\r\nIf xwavs were split, remake the LTSA in Triton\r\nUse a 5 sec/100 Hz resolution for odontocete analysis\r\nName the LTSA LL0##_FR##_DL##_5s_100Hz_splitFiles.ltsa and save within the xwavs folder\r\n\r\nMake a finer resolution LTSA for noise analysis\r\nUse a 2 sec/10 Hz resolution\r\nName the file LL0##_FR##_DL##_2s_10Hz.ltsa and save within the xwavs folder\r\n\r\nFinalize the README and phase_3_multi_deployments_summary.xslx by ensuring all inconsistencies are noted, and data checks completed, and all processing steps are done\r\nRun workflow_database_pullVesselIDs.R key for working with LOTUS database (limited to those with signed NDA/LOTUS access)\r\nBack to top\r\nAcoustic analyses\r\nCreate GitHub issue\r\nCreate a new issue to add this task to the Longline Acoustic Monitoring Project\r\nNavigate to the llamp repository’s New Issue page and press the green Get Started button for the Analysis template\r\nUpdate the created issue name with the correct trip number, add any assignees, and add to the Longline Acoustic Monitoring Project\r\n\r\n\r\n\r\n\r\nFigure 2: Screenshot showing the new issue template selection page on GitHub\r\n\r\n\r\n\r\nOdontocetes\r\nIdentifying false killer whale vocalizations from the longline recordings involves three main steps: manual annotation of possible odontocete events, automated detection of clicks, whistles, and burst pulses, and classification of the events based on the outputs of the detection process to provide a probability of false killer whale or unidentified odontocete species.\r\nThe manual event annotation and automated detection steps can be done simultanously/in any order; they do not rely on one another. The classification steps require the outputs from both the manual annotation and automated detection so must be done last.\r\nEvent annotation\r\nCandidate odontocete events are manually logged in MATLAB-based program Triton. This identifies the actual areas of interest, or analysis units, for odontocete detection and classification.\r\nLogging in Triton\r\nUse the latest version of Triton hosted on Github: MarineBioAcousticsRC/Triton\r\nSuggest ‘cloning’ this repository to your local computer using GitHub/GitHub Desktop to be able to stay up to date with any changes\r\nIf you will be running analyses on a virtual machine, you can clone the repository to a folder on your user drive that will be accessible when remoted in to the virtual machine. After cloning, right-click on this repository from the list of repositories within GitHub Desktop and select ‘Create alias’ to add a ‘VM’ or ‘remote’ label to this repository to distinguish it from any local copies\r\n\r\nAdd Triton to the MATLAB path\r\nAdd the Logger Remora\r\nCreate a new log per trip/recorder (e.g., log_LL060_FR38_DL65.xls)\r\nManually browse the LTSA and mark start and end times of potential odontocete encounters\r\nUse the 5 sec, 100 Hz LTSA for odontocete sounds\r\nView 15-30 minute windows at a time\r\nAdjust brightness and contrast as needed for each deployment and change as needed throughout the deployment as the sound energy changes\r\nNote potential species ID as possible; while our focus is fkw, any notes about potential species are useful for any future work\r\n\r\nClean up and collapse final events into a .csv\r\nOpen the Triton-generated log in excel and copy to a new csv file\r\nSave .csv with naming convention LL0##_FR##_DL##_encounterTimes.csv\r\nMust include columns ‘start’, ‘stop’, ‘species’, and ‘id’\r\n‘id’ column should be manually added and include the trip string and just a sequential account of events (e.g., L064_FR52_DL59_1, L064_FR52_DL59_2…)\r\nA fifth notes column can be included if relevant\r\nThese column namings, and what is listed can lead to some issues/unexpected outcomes when exporting in PAMpal and predicting with banter so this might change!\r\n\r\nBack to top\r\nAutomated detection\r\nPamguard is used to detect individual clicks, whistles, and burst pulses for further analysis. For ease and to prepare for future analyses, Pamguard is run on the full set of recordings for a given trip. For speed, Pamguard could be run just across the odontocete encounters above, but it generally runs fairly quickly and so is simpler to just run over the full xwav folder.\r\nPamguard setup\r\nUse Pamguard version 2.02.07b.\r\nTypically, the latest Pamguard version is available at pamguard.org but in this case we had a special bug fix release/version and so sourced files from JLKM.\r\nInstalling and configuring Pamguard has a few specific requirements, so if it is a new install, make sure it is set up correctly (note to self: increase memory, -smru tag, ensure is in a folder with read/write access)\r\n\r\nOpen Pamguard 2.02.07b\r\nNavigate to where this version of Pamguard is saved (typically C:\\Program Files\\pamguardbeta\\Pamguard_20207b or C:\\Program Files\\Pamguard\\Pamguard_20207b)\r\nPamguard has three ‘modes’; to run a detector initially, use the primary mode, just called Pamguard (it may be PamguardBeta depending on the installed version/installed folder location/installation naming convention)\r\nOpen the primary mode by double clicking Pamguard.exe\r\n\r\nAt startup, load the pam20207b_llharp_banter.psfx download here preferences file\r\nThis sets up the basic array configuration, sound acquisition, fft engine, and database modules that will need to be modified for each survey, plus the decimation process, click, whistle and moan, and cepstrum detector modules that should not be modified\r\nDetailed instructions for setting up this preferences file, with screenshots, can be found in the glider-MHI repo or talk to JLKM for more info!\r\n\r\nSave a copy of the preferences file specific for this survey before making any modifications\r\nFile > Save Configuration As …\r\nName the file the base file name plus the trip/recorder string e.g., pam20207b_llharp_banter_LL060_FR38_DL65.psfx\r\nSave this preferences file in pamguard folder within the LLHARP\\Analysis\\phase3 folder on the server\r\n\r\nModify the data input to the Sound Acquisition module\r\nSettings > Sound Acquisition …\r\nIn the pop up window, make sure the Data Source Type is set to Audio file folder or multiple files\r\nPress Select Folder or Files to browse for specific files\r\nNavigate to the xwavs folder that contains all .x.wav files for a single recorder, select the folder and press Select files and folders. It is ok if this folder also contains an .ltsa or other non .x.wav files\r\nThe File date: section should now show the start time of the first recording\r\nLeave all other boxes unchecked/as default and press Ok to close the Audio Data Acquisition settings window\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Screenshot showing the correct configuration of the Sound Aquisition module for an example folder of wav files\r\n\r\n\r\n\r\nSet the output database\r\nFile > Database > Database Selection\r\nPress Browse/Create …\r\nNavigate to the previously created pamguard folder\r\nIf it doesn’t already exist, create a databases folder within the pamguard folder\r\nNavigate to inside the databases folder and type the desired filename in the File name: field below and press Open\r\nName the database the same as the preferences file (but without the .psfx extension) e.g., pam20207b_llharp_banter_LL060_FR38_DL65\r\nThis long name long, yes, but is useful because it contains information about the Pamguard version used, the base preferences file, and what acoustic data this database specifically contains\r\n\r\nA pop-up will ask to ‘Create blank database [filename.sqlite3]’ and hit OK\r\nPress OK on the Database Selection window\r\n\r\n\r\n\r\n\r\n\r\nFigure 4: Screenshot showing the correct configuration for specifying the output database\r\n\r\n\r\n\r\nSet the output binaries folder\r\nFile > Binary Store > Storage Options …\r\nIn the Binary Store Options window press Browse\r\nNavigate to the pamguard folder\r\nIf it doesn’t already exist, create a binaries folder within the pamguard folder\r\nNavigate to inside the binaries folder and enter the desired binaries folder name in the Folder name: field below and press Select storage folder\r\nName the binaries the same as the database and preferences files e.g., pam20207b_llharp_banter_LL060_FR38_DL65\r\n\r\nPress Ok\r\nA pop up will appear stating ‘The directory [foldername] does not exist do you want to create it?’ and press Yes\r\nPress Ok again on the Binary Store Options window\r\n\r\nSave these changes with File > Save Configuration to save all the changes that have now been mapped to your PC and are specific for this survey\r\n\r\n\r\n\r\n\r\nFigure 5: Screenshot showing the correct configuration for the binary storage folder\r\n\r\n\r\n\r\nRun Pamguard\r\nEach recorder from each trip will be run individually and this will take several hours to run.\r\nPress the red circular record button to start processing the selected files/folder\r\nA spectrogram should start scrolling in the Spectrogram tab and colored symbols should start appearing on the Click Detection tab as clicks are detected\r\nClicks can be examined in real-time by clicking on individual symbols\r\nThe pause button at the top can be used to stop processing at the current file, and when the record button is pressed again it will resume where it previously paused\r\nIf the process is stopped and Pamguard is closed, when it is reopened the process would start at the first file in the folder, resulting in possibly duplicate results in the database and binaries\r\nBackup outputs\r\nEach time a database and/or binaries are opened in Pamguard Viewer after being created, they have the potential to be modified. This could overwrite or mess things up so it is good practice to always save an ‘original copy’ of the database and binaries before doing any additional analyses.\r\nMake a copy of both the database and folder of binaries and store on the data server in a folder called originals within the pamguard folder.\r\nBack to top\r\nClassification\r\nThe clicks, burst pulses, and whistles detected using Pamguard are grouped by event (from the manual annotation) and exported by R package PAMpal, then fed into the classification R package banter to try to identify which encounters were possibly FKW and which were likely other odontocete species.\r\nCloning code\r\nClassification code is located in the llamp repository in the R folder.\r\nBecause this code may be regularly updated, it is best to work directly with it using GitHub/GitHub Desktop. If you plan to make modifications to the code (for example, updating paths to a specific machine or workflow) then it is best to make a new ‘branch’ on this repository to have your own separate stream of edits. This ‘branch’ is an off-shoot of the original so you’ll be able to pull in updates committed to the original repo, and you’ll be able to make ‘pull requests’ to contribute any more broad changes to the original. But, with a branch, your changes won’t automatically get updated for others and it will avoid conflicts of multiple people working within the code at once. Creating a branch is only possible if you are a collaborator in the repository; if not, you can ‘fork’ the repository and still stay up-to-date with changes as well as make pull-requests.\r\nThe classification process also requires some functions located in the CRPTools repository. This repository only needs to be cloned to your local machine (first section below) because it will not be modified for a particular computer.\r\nBelow are suggested steps for cloning, creating a new branch, and working with GitHub/GitHub Desktop.\r\nTo clone a repository:\r\nOpen GitHub Desktop\r\nSelect File > Clone repository…\r\nSelect the URL tab and paste https://github.com/sfregosi/llamp into the empty URL space\r\nSpecify the location to save the repository on your local machine (suggest documents\\github\\llamp) and hit Clone\r\nThis will copy all the llamp components to the specified folder\r\nSelect this repository within GitHub Desktop from the Current repository drop down repository list on the left side\r\nTo create a new branch:\r\nThe default/primary branch is called main. In the Current branch drop down, select New branch and provide the branch a name (e.g., fregosi or picsn28 - the name of your computer)\r\nThis will create a new branch and and it’s name should now appear in the Current branch section in GitHub Desktop\r\nWhenever you want to switch between branches, you select it from this drop down in GitHub Desktop. But, once one branch is selected, it should remain the chosen branch through computer restarts, etc, but it is always good to double check if making major changes!\r\nFinally, hit Publish this branch to push this branch to the ‘remote’ location of this repository on github.com. This will only need to be done once after creating a new branch\r\nCommitting, pushing, and pulling changes:\r\nWhenever you make changes to code on your local machine, these changed files will appear in the left side panel of GitHub Desktop. After making a set of specific changes, you can create a snapshot of the files at that point in time (so you can go back if something bad happens, or others can easily track what was changed – the purpose of version control!) by making a ‘commit’ for these changes. Do so by typing a short message/description in the lower left and hitting Commit\r\nAfter committing changes, those changes have been saved on your local repository and branch, but ONLY locally. After making a commit, or several commits (e.g, at the end of the day), ‘push’ these changes to the remote repository on Github.com using the Push origin button on the top right of GitHub Desktop\r\nThe top-right will display different options depending on what changes are available\r\nIf there are local changes, it will display Push origin; it is good practice to push all commits at the end of each day or session working with a particular code set\r\nIf there are no local changes, it will say Fetch origin, which will check for new changes from the web\r\nIf there are changes present on the remote that are not yet local, it will say Pull origin\r\nIt is good practice to fetch and pull changes at the start of each day or session to avoid any conflicts of files being changed simultaneously in two locations. If you are working within your own branch, you will be the only one making changes so there won’t really be unexpected changes to be pulled from the web. But, if you are working within the same branch as others, or with code that you are not modifying but others are regularly updating, this becomes more critical\r\n\r\nMerging/updating branches\r\nSee this other page: branch-merge\r\nRunning classification\r\nUse the workflow_fkw_predict_phase3.R script to process, the manual events, Pamguard detections, and predict species ID with an existing classification model.\r\nOpen RStudio and open the llamp.rproj\r\nEither go to File > Open Project and navigate to the llamp folder and choose llamp.rproj or\r\nClick on the drop down in the top right of RStudio, select Open Project and navigate to the llamp folder and llamp.rproj file\r\nIf the project has been opened before, use the top right drop down to select llamp from the list of recent projects\r\n\r\nThe Files tab within RStudio should now show the llamp folder. Navigate to the R folder and open workflow_fkw_predict_phase3.R\r\nTo run individual lines of code in RStudio, put the cursor somewhere in the line and hit Ctrl + Enter. From there it will move you down to the next line so you can continually press Ctrl + Enter to move through the script\r\nEnsure the proper packages have been installed/updated and loaded\r\nMake needed changes in the USER DEFINED INPUTS section\r\nUpdate the trip number and recorder string for the dataset to be classified\r\nUpdate the paths as necessary to map to the data input and analysis output locations\r\npath_data and path_local are actually not needed but have been left just in case they are later. They can be commented out with #\r\npath_analysis should map to the folder that contains the pamguard folder which in turn contains the databases and binaries sub directories\r\npath_pg will be generated based on path_analysis\r\n\r\nRun all lines in this section after updating\r\n\r\nStep through all lines in the PAMpal steps section\r\nEnter 16 for bit rate when prompted during parameter object creation\r\nWhen running View(banter_dets) double check that events all match what was exported in the encounterTimes.csv. There should be a column for start, end, species, and id (unique event id)\r\nThis will save several .rda rdata object outputs in an eventData folder\r\n\r\nStep through all lines in the BANTER steps section\r\nFirst, load the existing model\r\nUpdate the path to the CRPTools repository if needed\r\nPredicted species labels will be saved in a .csv\r\n\r\nBack to top\r\nNoise\r\nUnderwater recordings\r\nThe primary question for this stage of noise analysis is just if/when it is present in the underwater recordings. The best way to identify the noise in recordings is from manual scanning of LTSAs.\r\nCreate finer-resolution LTSA in Triton using 2 sec, 10 Hz averaging for noise analysis\r\nManually scan the LTSA, plotting from 0 Hz to 5 or 10 kHz, and 2-4 hours at a time\r\nMark the start and end noise of any possible gear noise\r\nIf the noise is detected, we may conduct additional manual and automated analyses in Pamguard to identify individual clusters and quantify the timing of these clusters.\r\n\r\n\r\n\r\n\r\nFigure 6: Example LTSA showing the gear noise of interest. The noise is present around 1.5 kHz from the 0.2 to 1.8 hours, with the loudest part centered around 0.6 hours\r\n\r\n\r\n\r\n THIS SECTION UNDER CONSTRUCTION\r\nAdditional automated approaches have been done in the past, but we are holding off on those for now, because they weren’t as reliable as manual scanning. These include the Pamguard Noise Band Monitor Module (using the pam20207b_llharp_noise.psfx preferences file - download here) and SanctSound metrics (both third octave level approaches).\r\n\r\n\r\n\r\n\r\nFigure 7: Screenshot showing the data model for the pam20207b_llharp_noise.psfx preferences file, which runs a Noise Band Monitor process\r\n\r\n\r\n\r\nIn-air recordings\r\n THIS SECTION UNDER CONSTRUCTION\r\nUnder development\r\n- Review Zoom recordings to characterize sound\r\nBack to top\r\nPast protocols\r\nLLHARP Part III - Data Analysis Google DocLast updated 24 March 2017\r\n- Manual scanning of LTSAs in Triton to find bouts, whistle classification using Raven to generate manual whistle picks plus ROCCA to trace and classify, and click detection and classification using a a set of Matlab tools (appears to be Triton-based but not sure on specific versions)\r\n- This is the approach used in Bayless et al. 2017\r\nBack to top\r\nLast updated\r\n 15 March 2023\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-15T20:27:18-07:00"
    },
    {
      "path": "branch-merge.html",
      "title": "How To for Merging Branches",
      "description": "Guide for updating a 'personal' GitHub branch with the main branch\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nBackground\r\nUpdating branch - only behind main\r\nUpdating a branch - only ahead of main\r\nUpdating branch - ahead and behind\r\nLast updated\r\n\r\n\r\n\r\n\r\nBackground\r\nWhen working with a code repository that may receive regular updates, but that you need to modify locally to make minor changes (such as updating paths, etc) you can either fork the repository or create a branch. If you are not a collaborator with direct modification access to the repository, you will need to fork the repository. If you are considered a collaborator (by GitHub permissions) than you can create a branch.\r\nThe best approach for this project (I think) is to create a new branch to have your own separate stream of edits for information like paths, folder structure, etc. This ‘branch’ is an off-shoot of the original, which means you can pull in updates committed to the main branch of the repo but maintain your own edits. Additionally, you’ll be able to make ‘pull requests’ to contribute any broad changes that would benefit the original (bug fixes, enhancements). But, with a branch, your changes won’t automatically get updated for others and it will avoid conflicts of multiple people working within the code at once.\r\nUpdating branch - only behind main\r\nIf the main branch has updates that you wish to encorporate into your branch, you will do this through the GitHub website.\r\nNavigate to the repository of interest\r\nSelect your branch from the drop down menu on the upper left\r\nGitHub will tell how many commits ahead and how many commits behind this branch is from the main branch\r\nIf your branch is only commits behind the main branch, updating should be fairly straightforward\r\n\r\n\r\n\r\n\r\nFigure 1: This screenshot shows the selected branch (not main, but in this case SFs personal branch, picsn28), and how it is 2 commits behind the main branch\r\n\r\n\r\n\r\nClick on the link with the number of commits behind (in the above screenshot, where it says ‘2 commits behind’ in blue), to open the branch comparison page\r\nThe compare page will note the specific commits that are different, and check if the two branches can be merged without any conflicts (e.g., edits on the same lines in the same files)\r\n\r\n\r\n\r\n\r\nFigure 2: The compare page spells out the exact changes, and checks the ability to merge\r\n\r\n\r\n\r\nIf there are no merge conflicts, select the green Create pull request button\r\nGive the pull request a short description\r\nIn the pull request pag, there is more detailed checks, a statement about conflicts, and space to include additional comments\r\n\r\n\r\n\r\n\r\nFigure 3: This screenshot shows the pull request page, in a case with no merge conflicts\r\n\r\n\r\n\r\nIf there are no conflicts, you can select the green Merge pull request button and all commits from the main branch will be merged with the current, selected branch\r\nThe branch that main was merged with will now say that it is ‘1 commit ahead of main’ because of this merge commit\r\nBack to top\r\nUpdating a branch - only ahead of main\r\nUpdating branch - ahead and behind\r\nLast updated\r\n 15 March 2023\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-15T20:28:20-07:00"
    },
    {
      "path": "disclaimer.html",
      "title": "disclaimer",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nLongline Acoustic Monitoring Project Hub\r\n\r\n\r\nHome\r\n\r\n\r\nProtocols\r\n \r\n▾\r\n\r\n\r\nVessel facilitation\r\nHardware\r\nData processing\r\nAnalysis\r\nReporting\r\n\r\n\r\n\r\n\r\nOther\r\n \r\n▾\r\n\r\n\r\nSummary\r\nDrive docs\r\n\r\n\r\n\r\n\r\n\r\n☰\r\n\r\n\r\n  \r\n    \r\n      \r\n        \r\n        \r\n        \r\n      \r\n      \r\n    \r\n    \r\n      \r\n  Home\r\n\r\n\r\n  \r\n    Protocols\r\n     \r\n    \r\n  \r\n  \r\n      Vessel facilitation\r\n    \r\n    \r\n      Hardware\r\n    \r\n    \r\n      Data processing\r\n    \r\n    \r\n      Analysis\r\n    \r\n    \r\n      Reporting\r\n    \r\n  \r\n\r\n  \r\n    Other\r\n     \r\n    \r\n  \r\n  \r\n      Summary\r\n    \r\n    \r\n      Drive docs\r\n    \r\n  \r\n\r\n  \r\n    \r\n     \r\n  \r\n\r\n      \r\n  \r\n\r\n\r\n\r\n\r\n\r\n\r\ndisclaimer\r\n\r\n\r\n\r\n\r\n\r\nDisclaimer\r\n“The United States Department of Commerce (DOC) GitHub project code\r\nis provided on an ‘as is’ basis and the user assumes responsibility for\r\nits use. DOC has relinquished control of the information and no longer\r\nhas responsibility to protect the integrity, confidentiality, or\r\navailability of the information. Any claims against the Department of\r\nCommerce stemming from the use of its GitHub project will be governed by\r\nall applicable Federal law. Any reference to specific commercial\r\nproducts, processes, or services by service mark, trademark,\r\nmanufacturer, or otherwise, does not constitute or imply their\r\nendorsement, recommendation or favoring by the Department of Commerce.\r\nThe Department of Commerce seal and logo, or the seal and logo of a DOC\r\nbureau, shall not be used in any manner to imply endorsement of any\r\ncommercial product or activity by DOC or the United States\r\nGovernment.”\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-01T09:22:42-08:00"
    },
    {
      "path": "drive-links.html",
      "title": "Google Drive Links",
      "description": "A slightly more organized way to get to various docs saved in Google Drive\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nMetadata\r\nVessel recruitment\r\nHardware\r\nData processing\r\nAnalysis\r\nReporting\r\nLast Updated\r\n\r\nMost (all?) of these live in various folders within the LLHARP Master Google Drive folder.\r\nProject Management: Google Doc with outline of all project steps/components and who is in charge of what. This is somewhat outdated as of January 2023, and is meant to be replaced by this hub!\r\nMetadata\r\nLLHARP Data Summary: Metadata for every trip including hardware and software versions, data size and records of data uploads, and notes on issues or concerns. This document does contain vessel names and trip dates so access is restricted to those it has been shared with through GDrive.\r\nLongline HARP Deployments:\r\nRecord of trip numbers, vessels, and captain/owner/observer contact information. This contains vessel names, trip dates, and personal info so it is restricted to only being shared within our group.\r\nAcoustic Server Data Organization: Detailed list of server locations for all acoustic data, including LLHARP data. This document is restricted to only being shared within our group.\r\nVessel recruitment\r\nConfirmed Vessel List: This contains vessel names and contact information for owners and captains that have volunteered or have previously taken gear. This contains personal info for the vessels and so has restricted access to only those it has been shared with.\r\nHardware\r\nLLHARP Tracker 2.0: Spreadsheet to keep track of gear status (hydrophones, cases, etc). This doc is shared with SIO.\r\nTDR_CRP_Usage: Spreadsheet to track status of TDRs and record of deployments; this doc is also used for DASBRs.\r\nZoom Recorder Tracker: Spreadsheet to track status/location of Zoom recorders.\r\nLongline HARP User Manual April 2021\r\nBlank metadata check sheet\r\nInstructions for Deployment Set-Up and Recovery: This is a very thorough protocol document that can be eventually converted to Rmd. Last updated 13 August 2021\r\nold protocol LLHARP Part I - Setup, Deploy, Recover.doc: Older protocol, last updated 17 March 2017\r\nData processing\r\nold protocol LLHARP Part II - Data Processing Google Doc 9 August 2022\r\nAnalysis\r\nAll Pamguard preferences files, etc have been moved to GitHub, to the private llamp repository. Create an issue to request access.\r\nReporting\r\nHARP Noise of Interest Captain Feedback Log: Some older notes on ideas of what the noise might be, collected by Colby at Protected Species trainings.\r\nLast Updated\r\n 15 March 2023\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-14T21:01:38-07:00"
    },
    {
      "path": "hardware.html",
      "title": "Protocol - Hardware",
      "description": "All protocols and resources related to hardware live here\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nInventory tracking\r\nSetup - before deployment\r\nBreakdown - after recovery\r\nExisting protocol can be found here:\r\nLast Updated\r\n\r\nInventory tracking\r\nLLHARP Tracker 2.0\r\nMulti-tabbed Google Sheets workbook to inventory for all individual LLHARP parts as well as assembled systems that are either ready for deployment or are currently deployed\r\nThe three main ‘swappable’ components each have their own tab: frames, dataloggers, and hydrophones\r\nEach tab has a Location and a Status column that can be used to indiciate if the part is in house at IRC or is at SIO for repair, and if the part is currently deployed, ready to go, or is awaiting repairs. Additional notes on issues and needed repairs can be made for each part\r\nTDR_CRP_Usage\r\n- Track status of TDRs. This is for all of CRP. Ok to continue to live here!\r\nZoom Recorder Tracker\r\nLLHARP tracker Google Sheet original version shared with SIO for keeping track of gear status\r\nk\r\nBack to top\r\nSetup - before deployment\r\nIf not done already, create a New Issue using the Hardware Setup template\r\nProgram and assemble LLHARPs\r\nAmazingly thorough (and fun!) Instructions for Deployment Set-Up and Recovery by Y. Barkley Last updated 13 August 2021\r\nCurrent version of the Longline HARP User Manual here\r\nNote any configuration changes in the LLHARP Tracker 2.0\r\n\r\nProgram and attach TDRs\r\nFill out TDR usage sheet with start time and sampling schedule\r\n\r\nFill out deployment check sheet for each instrument assembled (blank metadata check sheet)\r\nPrep in-air recorders (See In-air Recorder Prep doc)\r\nPre-deployment section: new batteries, wipe and install SD cards, tape up safely, pack attaché\r\nUpdate Zoom Recorder Tracker\r\n\r\nCreate empty folder on server in \\\\PICQUEENFISH\\PSD2\\CRP\\LLHARP\\ for this trip by making a copy of the LL999 Template - Make a Copy! folder\r\nBreakdown - after recovery\r\nIf not done already, create a New Issue using the Hardware Breakdown template\r\nInitial data check\r\nTDR stop and offload\r\nUpload data to the the server (\\\\PICQUEENFISH\\PSD2\\CRP\\LLHARP\\LL0##\\); TDR data files go in the DL##-TDR## folder within each of the FR##_DL## folders\r\nFill out TDR usage sheet for stop time\r\n\r\nBreakdown LLHARPs\r\nOpen up and extract SD cards and hand off to data processor\r\nUpdate LLHARP Data Summary with power down date and voltages\r\nUpdate LLHARP Tracker 2.0 with any noted issues with any components\r\n\r\nBreakdown in-air recorders (See In-air Recorder Prep doc)\r\nOpen up and extract SD cards and remove batteries\r\nEither hand off SD cards to data processor or upload to server (\\\\PICQUEENFISH\\PSD2\\CRP\\LLHARP\\LL0##\\InAirRecorder\\); Zoom data files are each placed within a Z0# folder\r\nUpdate Zoom Recorder Tracker\r\n\r\nScan deployment and recovery check sheets and upload to trip folder on the server, named LL0## metadata sheets.pdf\r\nScan observer datasheets and upload to trip folder on the server, named LL0##_observer_sheets_scanned.pdf\r\nThe scanner seems to automatically ‘enhance’ the sheets which makes some of them unreadable\r\nIf the scans are not readable, take photos of the each observer datasheet and place the raw photos in the Observer Sheet Photos folder within the main LLHARP folder for further processing\r\nRelevant doc: Unreadable bits to check\r\n\r\nExisting protocol can be found here:\r\nprep and offload\r\nAmazingly thorough (and fun!) Instructions for Deployment Set-Up and Recovery Google Doc by Y. Barkley 13 August 2021\r\nLLHARP Part I - Setup, Deploy, Recover.doc Google Doc 17 March 2017\r\nBack to top\r\nLast Updated\r\n 15 March 2023\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-14T21:01:38-07:00"
    },
    {
      "path": "index.html",
      "title": "Longline Acoustic Monitoring Project",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          \r\n          \r\n          Longline Acoustic Monitoring Project Hub\r\n          \r\n          \r\n          Home\r\n          \r\n          \r\n          Protocols\r\n           \r\n          ▾\r\n          \r\n          \r\n          Vessel facilitation\r\n          Hardware\r\n          Data processing\r\n          Analysis\r\n          Reporting\r\n          \r\n          \r\n          \r\n          \r\n          Other\r\n           \r\n          ▾\r\n          \r\n          \r\n          Summary\r\n          Drive docs\r\n          \r\n          \r\n          \r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Longline Acoustic Monitoring Project\r\n            \r\n            \r\n              \r\n                \r\n            \r\n          \r\n        \r\n        \r\n        \r\n          \r\n            \r\n            About this site\r\n            This repo is Longline Acoustic Monitoring Project Hub - a\r\n            central location for documentation and guides to data\r\n            organization.\r\n            The web address for this repository is: sfregosi.github.io/llamp-hub\r\n            This repository and site are public, but a more\r\n            extensive private llamp\r\n            repository contains additional code and analyses.\r\n            \r\n            Site Contents\r\n            Summary -\r\n            High level summary of work conducted so far\r\n            Protocols - Top left menu to detailed\r\n            protocols for specific components/steps of the project:\r\n            vessel recruitment, hardware, data processing, analysis, and\r\n            reporting.\r\n            Drive\r\n            docs - links and descriptions of important docs\r\n            and sheets on Google Drive. Access to individual links is\r\n            restricted based on Google Drive sharing settings.\r\n            \r\n            \r\n            \r\n            \r\n            About this project\r\n            The Longline Acoustic Monitoring Project (recently dubbed\r\n            LLAMP) is a collaborative project between NOAA NMFS PIFSC\r\n            and the Hawaiian longline fleet to better understand how and\r\n            why false killer whales interact with the fishery, and try\r\n            to reduce these interactions.\r\n            This project has been going on for many years and\r\n            involves many wonderful people!\r\n            \r\n            DISCLAIMER:\r\n            The scientific results and conclusions, as well as\r\n            any views or opinions expressed herein, are those of the\r\n            author(s) and do not necessarily reflect the views of NOAA\r\n            or the Department of Commerce.\r\n             This repository is a scientific product and is not\r\n            official communication of the National Oceanic and\r\n            Atmospheric Administration, or the United States Department\r\n            of Commerce. All NOAA GitHub project code is provided on an\r\n            ‘as is’ basis and the user assumes responsibility for its\r\n            use. Any claims against the Department of Commerce or\r\n            Department of Commerce bureaus stemming from the use of this\r\n            GitHub project will be governed by all applicable Federal\r\n            law. Any reference to specific commercial products,\r\n            processes, or services by service mark, trademark,\r\n            manufacturer, or otherwise, does not constitute or imply\r\n            their endorsement, recommendation or favoring by the\r\n            Department of Commerce. The Department of Commerce seal and\r\n            logo, or the seal and logo of a DOC bureau, shall not be\r\n            used in any manner to imply endorsement of any commercial\r\n            product or activity by DOC or the United States Government.\r\n            \r\n            Last Update: 15 Mar 2023\r\n            \r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Longline Acoustic Monitoring Project\r\n            \r\n            \r\n              \r\n                \r\n                                  \r\n              \r\n            \r\n            \r\n              \r\n              About this site\r\n              This repo is Longline Acoustic Monitoring Project Hub -\r\n              a central location for documentation and guides to data\r\n              organization.\r\n              The web address for this repository is: sfregosi.github.io/llamp-hub\r\n              This repository and site are public, but a\r\n              more extensive private llamp\r\n              repository contains additional code and analyses.\r\n              \r\n              Site Contents\r\n              Summary -\r\n              High level summary of work conducted so far\r\n              Protocols - Top left menu to detailed\r\n              protocols for specific components/steps of the project:\r\n              vessel recruitment, hardware, data processing, analysis,\r\n              and reporting.\r\n              Drive\r\n              docs - links and descriptions of important\r\n              docs and sheets on Google Drive. Access to individual\r\n              links is restricted based on Google Drive sharing\r\n              settings.\r\n              \r\n              \r\n              \r\n              \r\n              About this project\r\n              The Longline Acoustic Monitoring Project (recently\r\n              dubbed LLAMP) is a collaborative project between NOAA NMFS\r\n              PIFSC and the Hawaiian longline fleet to better understand\r\n              how and why false killer whales interact with the fishery,\r\n              and try to reduce these interactions.\r\n              This project has been going on for many years and\r\n              involves many wonderful people!\r\n              \r\n              DISCLAIMER:\r\n              The scientific results and conclusions, as well as\r\n              any views or opinions expressed herein, are those of the\r\n              author(s) and do not necessarily reflect the views of NOAA\r\n              or the Department of Commerce.\r\n               This repository is a scientific product and is\r\n              not official communication of the National Oceanic and\r\n              Atmospheric Administration, or the United States\r\n              Department of Commerce. All NOAA GitHub project code is\r\n              provided on an ‘as is’ basis and the user assumes\r\n              responsibility for its use. Any claims against the\r\n              Department of Commerce or Department of Commerce bureaus\r\n              stemming from the use of this GitHub project will be\r\n              governed by all applicable Federal law. Any reference to\r\n              specific commercial products, processes, or services by\r\n              service mark, trademark, manufacturer, or otherwise, does\r\n              not constitute or imply their endorsement, recommendation\r\n              or favoring by the Department of Commerce. The Department\r\n              of Commerce seal and logo, or the seal and logo of a DOC\r\n              bureau, shall not be used in any manner to imply\r\n              endorsement of any commercial product or activity by DOC\r\n              or the United States Government. \r\n              Last Update: 15 Mar 2023\r\n              \r\n              \r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2023-03-14T21:01:38-07:00"
    },
    {
      "path": "processing.html",
      "title": "Protocol - Data Processing",
      "description": "Details on data processing steps following recorder recovery\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nCreate GitHub Issue\r\nDetailed data processing steps\r\nUpdate metadata\r\nImage HRP\r\nProcess HRP - 2020 processing code\r\nProcess HRP - 2022 processing code\r\nBackground details\r\nSet up the processing machine\r\nHRPproc menu options\r\nHRP File Tools\r\nProcessing Tools\r\nHRP Debug Tools\r\n\r\n\r\nUpload data\r\n\r\nPrevious/existing documentation\r\nLast Updated\r\n\r\n\r\n\r\n\r\n\r\nCreate GitHub Issue\r\nCreate a new data processing issue for the latest trip, and add it to the Longline Acoustic Monitoring GitHub Project.\r\nNavigate to the llamp repository’s New Issue page and press the green Get Started button for the Data processing template\r\nUpdate the issue name to match this trip number, add assignees, and add to the Longline Acoustic Monitoring Project\r\n\r\n\r\n\r\n\r\nFigure 1: Screenshot showing the new issue chooser with just the Post-processing template shown. Select the ‘Data processing template’\r\n\r\n\r\n\r\nBack to top\r\nDetailed data processing steps\r\nUpdate metadata\r\nFill in relevant deployment info on LLHARP Data Summary\r\nInfo to be filled out:\r\nFrame and Datalogger numbers, versions, settings, dates from the check sheets\r\nTDR number\r\ny/n for upload of datasheets\r\nAny set-up/off-load problems\r\nBack to top\r\nImage HRP\r\nImage Longline HARP Data (create .hrp file from SD card).This is a several-hours long process, and should be completed locally on an external HD\r\nConnect both the SD card, and the hard drive that you are copying the data to\r\nThe SD card will show up as an unformatted drive. Do NOT format it\r\n\r\nOpen MATLAB and run dd.exe --list as a command line command to list all of the attached disks\r\nA copy of dd.exe can be found on Google Drive at ‘LLHARP MASTER/Data Management Docs/LLHARP_processing_code_2020/dd-0.5/’\r\nExample:\r\n!C:\\Users\\Ann.Allen\\Documents\\MATLAB\\SIOHARPData_processing_codes_20170403\\dd-0.5\\dd.exe --list\r\nNote the HarddiskVolume# for the SD card. If it is not obvious which one the SD card is, disconnect the card, re-run --list, and then reconnect and look for the newly added Harddisk and Partition\r\n\r\n\r\n\r\n\r\nFigure 2: Screenshot showing output for listing the partitions\r\n\r\n\r\n\r\nCreate .hrp file\r\nEnter if=, of=, and bs= commands to start imaging, where\r\nThe first path is the local path where you have dd.exe stored\r\nif= is the path of the SD card from the last step\r\nof= is the drive path for where you want to save the .hrp file and what you want to name it\r\nThe .hrp file naming will follow the convention: dataID_TwoLetterBoatAbbreviation_FourLetterFrameNumber_FourLetterDataLoggerNumber.hrp\r\n\r\nbs=1M\r\n\r\nExample:\r\n!C:\\Users\\Ann.Allen\\Documents\\MATLAB\\SIOHARPData_processing_codes_20170403\\dd\\dd.exe if=\\\\?\\Device\\HarddiskVolume34 of=DRIVE:\\LL008\\LL008_KM_FR03_DL73.hrp bs=1M\r\nThis will create one large .hrp file that can then be converted into multiple separate xwav files in Triton (see below ‘Process HRP tasks’)\r\nThe .hrp file takes several hours to offload. There will not necessarily be any indication that the process is working besides a continual ‘Busy’\r\nmessage in MATLAB and the size of the .hrp file being generated will not necessarily increase while the process is working\r\nWhen processing is finished your screen should look similar to Figure 3 below\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Screenshot showing what the imaging process should look like after running the dd.exe\r\n\r\n\r\n\r\nCopy raw HRP data to server (\\\\PICQUEENFISH\\PSD2\\CRP\\LLHARP\\LL0##\\); each HRP goes in the HRP folder within each of the FR##_DL## folders\r\nBack to top\r\nProcess HRP - 2020 processing code\r\nCreate Parameter File\r\nThis is a .txt file with deployment information\r\nThe naming convention follows the .hrp file (e.g., ’LL0##_XX_FR##_DL##.txt’ where XX is the two letter vessel abbreviation) but with ‘.txt’ as the extension\r\nIt is easiest to (a) copy the file from a previous deployment and rename according to same convention as the .hrp file or (b) copy the block of text below into a new blank text file, then update accordingly:\r\nExperiment Name (8 chars) = Data ID based on trip # & vessel ID (ex. ‘LL001_KM’)\r\nSite Name (4 characters) = Frame number (ex. FR03)\r\nInstrument ID (4 characters) = Datalogger ID (ex. DL73)\r\nLatitude, Longitude and Depth do not need to be added\r\n\r\nSave the updated file to the same folder as the .hrp file\r\n\r\n\r\n\r\n\r\nMATLAB Setup\r\nMake sure that the folder ‘LLHARP_processing_code_2020’ is in a local directory and on the MATLAB path\r\nIf you have any other Triton folder on your MATLAB path, make sure that this folder is higher in the directory\r\nIf there are new versions of the firmware on the LLHARP you will need to get a new version of ‘ckFirmware.m’ and ‘table_ckFirmware.csv’\r\nRun Triton by typing triton in the MATLAB command window\r\nIf it is not already installed, add the ‘HRP’ Remora to Triton\r\n\r\n\r\nProcess the .hrp file\r\nProcess using the HRP Remora\r\nRemoras > HRP File > Convert HRP File to XWAV files\r\nSelect ‘.hrp file’\r\nSelect the parameter file that you created (It has a .txt extension so you will need to select ‘view all files’ in the GUI)\r\nSelect the output folder for the XWAVs\r\nIf there are no errors the MATLAB output will look like:\r\n\r\n\r\n\r\n\r\n\r\nFigure 4: Screenshot showing output as it should be if there are no errors while processing the HRP to xwavs\r\n\r\n\r\n\r\nMake the LTSA\r\nWithin Triton select ‘Tools > Make LTSA from Directory of Files’\r\nEnter file type: 2 (XWAV)\r\nSelect directory with XWAV files\r\nSet Long-term spectrogram parameters:\r\ntime average length (seconds): 5\r\nFrequency bin size (Hz): 100\r\n\r\nLTSA should be named to include the trip ID, instrument ID and LTSA info\r\nExample: ‘LL008_FR04_5s_100Hz.ltsa’\r\n\r\n\r\nThis process will take several hours\r\n\r\nBack to top\r\nProcess HRP - 2022 processing code\r\nBackground details\r\nStarting with LL065, SF trying to process the LLHARP with the latest processing code from SIO. SF initially received this version via email from A. Allen/E O’Neill and it was placed in a private GitHub repo harp-processing-sio-2022. The 2020 processing code originally was on Google Drive (still there here) but SF also made that into a GitHub repository in case any changes have to be made to it, so it is now under version control: longline-processing-2020\r\nSet up the processing machine\r\nDepending on where the processing is happening, different machines may have different saved MATLAB paths that point to different versions of Triton which can lead to issues. Start by properly setting up the correct MATLAB paths.\r\nFirst, set Matlab path to the default\r\nGo to Set Path in top ribbon menus (in the HOME tab)\r\nSelect Default, and say Yes when prompted to confirm\r\nPress Save to save the settings and then Close to exit the Set Path window\r\n\r\nSecond, set Matlab path to the harp-processing-2022 repository\r\nGo to Set Path in top ribbon menu (in the HOME tab)\r\nSelect Add with Subfolders… and navigate to the correct local copy of the repository on this machine and press Select Folder\r\nOn the virtual machine PICV023, this location is \\\\PICKINGFISH\\USERS\\selene.fregosi\\CodeForVMs\\harp-processing-sio-2022\r\n\r\nA bunch of paths will now be listed at the top of the list, highlighted in blue\r\nPress Save and Close to set the path\r\n\r\nAfter setting the paths, run Triton and add the HRP Remora\r\n- Run Triton by typing triton in the MATLAB Command Window\r\n- The text that populates the Command Window should say the below version information if the correct path/Triton version are set:\r\n   \r\n         Triton version 1.94.20220401\r\n \r\nClearing all windows to begin Triton\r\nThe three Triton windows will open and the correct version (1.94.20220401) should also be displayed in the Plot - Triton window with the logo\r\nIf this is the first time processing has been run on this machine, the HRPproc Remora will have to be added\r\nIn the Control- Triton window, select Remoras > Add Remora\r\nNavigate to the ‘triton.194.20220401/Remoras’ folder, highlight the ‘HARPproc_220401’ folder, and ‘Select Folder’\r\nNB: I do not know what the difference between ‘HARPproc_220401’ and ‘HARPproc_220401_X2’ is\r\nSelect ‘Yes’ when prompted to restart Triton\r\nIf the Remora was properly added, then it will now be visible in the Remoras drop down menu in the Control - Triton window\r\n\r\nHRPproc menu options\r\nHRP File Tools\r\nDisk Header: Provides disk header information for a select HRP file and prints it into the Message - Triton folder. Example output:\r\nfiletype 1: USB file\r\n \r\nSector 0: \r\nDisk Type = HARP\r\nDisk Number = 2\r\n \r\nSector 2: \r\nFirst Directory Location [Sectors] = 8\r\nCurrent Directory Location [Sectors] = 226\r\n \r\nFirst File Location [Sectors] = 539\r\nNext File Location [Sectors] = 104816111\r\n \r\nMax Number of Files = 8326\r\nNext File = 3491\r\n \r\nSample rate = 200000\r\nDisk Number = 2\r\nFirmware Version = V2.34\r\nDescription = No Decsription entered\r\nDisk Size [Sectors] = 249737216\r\nDirectory List: Lists all raw files ona selected HRP. List is printed directly into the MATLAB Command Window and contains 12 columns. The columns are: raw file number, ??, 2 digit year, month, day, hour, min, sec, millisecond, sample rate, ??, ??, ??.\r\nExample output:\r\n\r\n   1        539         22          8         27          0          8         11        520     200000      30000   15360000          0\r\n   2      30539         22          8         27          0          8          4        600     200000      30000   15360000          0\r\n   3      60539         22          8         27          0          7         57        680     200000      30000   15360000          0\r\n   4      90539         22          8         27          0          7         50        760     200000      30000   15360000          0\r\n   5     120539         22          8         27          0          7         43        840     200000      30000   15360000          0\r\n   6     150539         22          8         27          0          8         58        840     200000      30000   15360000          0\r\n\r\n...\r\n\r\n3486  104631743         22          9          4         13         24         16          0     200000      30000   15360000          0\r\n3487  104661743         22          9          4         13         25         31          0     200000      30000   15360000          0\r\n3488  104691743         22          9          4         13         26         46          0     200000      30115   15418880          0\r\n3489  104721858         22          9          4         13         28          1          0     200000      30004   15362048          0\r\n3490  104751862         22          9          4         13         29         16          0     200000      30240   15482880          0\r\n3491  104782102         22          9          4         13         30         31          0     200000      34009   17412608          0\r\n\r\nProcessing Tools\r\nHRP to XWAV: Select this menu option to convert a single HRP to its individual xwavs.\r\nWill open new window and prompt to ‘Load HRP Processing Parameters’. Previously this information was read in from a text file but in this case it will load a previously created/saved .mat file.\r\nFor the first time processing a HRP, select No\r\nFrom what I can tell in the code, Yes would allow you to load a previously created/saved .mat file, but see below, Save doesn’t seem to work\r\nYes; resume processing doesn’t look like it loads any parameters, so must just use what is currently loaded/set\r\n\r\nNew GUI window will open. Set decimation factor (DF) to 1 and hit OK\r\nNext window prompts to Browse for xwav output folder. save within the folder for each FR##_DL## for this trip. After selecting folder, hit Continue\r\nNew window allows additional options to be selected. Not sure what these do yet…\r\nSave params button throws an error in hrp2xwav_paramC.m\r\nTHINK this is fixed with this commit\r\n\r\nContinue does work\r\n\r\nSelect the HRP to be processed in the next pop up window\r\nEither load or specify the trip details in the next GUI\r\nTo just load a .txt hdr file as built above, just type Y in first entry and hit OK and you will be prompted to select the appropriate file\r\nOtherwise, specify the values in each entry line and hit OK\r\nIf any inputs are the wrong size or format, a warning box will appear and you will be able to re-enter or re-select the appropriate .txt file\r\n\r\nIt will then prompt to again save the parameters and suggest a particular folder within the data folder\r\nIn the future, might make sense to save an earlier version of the more simple parameters in a higher level folder (e.g., the decimation factor, etc) and call that for LLHARPs in general, than save specific ones for each trip and frame that has the full paths mapped. Need to look into this more\r\n\r\nXWAV to LTSA:\r\nHRP Debug Tools\r\nThese are all ‘greyed’ out\r\nCheck Directory List Times\r\nPlot Sector Times\r\nFix Directory List Times\r\nBack to top\r\nUpload data\r\nput copy of HRP and xwavs on server\r\nsave copy of HRP on physical drive (and note this somewhere…)\r\nput in-air recordings on server\r\nBack to top\r\nPrevious/existing documentation\r\nLLHARP Part II - Data Processing\r\nGoogle Doc 9 August 2022\r\nBack to top\r\nLast Updated\r\n 15 March 2023\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-14T21:01:38-07:00"
    },
    {
      "path": "reporting.html",
      "title": "Protocol - Reporting",
      "description": "Details on reporting steps to be completed after analysis\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nLast Updated\r\n\r\nThis protocol is very much a work in progress \r\nFor now, all reporting is done manually following a powerpoint template that lives locally on Selene’s computer. BUT…she has dreams of automating this with RMD!\r\nSome notes from local word docs:\r\nPlot TDR and spatial data for reporting\r\nLast Updated\r\n 15 March 2023\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-14T21:01:38-07:00"
    },
    {
      "path": "summary.html",
      "title": "Project Summary",
      "description": "High level summary of this projects past and ongoing efforts and outcomes\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPhase I\r\nPhase II\r\nPhase III - Multiple Recorders\r\nLast Updated\r\n\r\nSummary docs quick links\r\nLLHARP Data Summary Google Sheet\r\nPhase I\r\nCharters with multiple recorders\r\nThis initial phase of the project developed the recorder technology and worked with a chartered fishing vessel to fine-tune the equipment and monitor several fishing trips with up to five recorders on each fishing set.\r\nMajor outcome:Bayless et al. 2017 which quantifies how often, and during what part of the fishing set interactions happen.\r\n\r\n\r\n\r\n\r\nFigure 1: Tables 1 and 2 from Bayless et al. 2017\r\n\r\n\r\n\r\nBack to top\r\nPhase II\r\nSingle recorders\r\nFollowing several initial chartered trips, we worked with volunteer fishing vessels to take out a single recorder on trips in which those vessels had observer coverage. Through this phase we had a total of 59 fishing trips with recorders deployed.\r\nA manuscript is currently in prep by Simonis et al. that expands on Bayless et al. 2017 and includes results from these single-recorder trips as well as the charter trips. It provides an automated detection and classification system for identifying false killer whales from LLHARP recordings and describes a particular noise of interest that is correlated with increased false killer whale detections in detail.\r\n\r\n\r\n\r\nBack to top\r\nPhase III - Multiple Recorders\r\n More to come here!\r\nThis most recent phase of the project was launched in early 2021 and renewed efforts to deploy multiple recorders across the gear within a single fishing set. We continue to work with volunteer vessels to take out up to four recorders on trips where they have observer coverage. We are working to better understand what exactly is generating the specific noise of interest identified in Phase II, and potential mitigation methods. This effort includes two new aspects: (1) working with collaborators at Scripps Institution of Oceanography to model sound propagation around these fishing vessels and (2) taking in-air sound samples of two pieces of equipment on each vessel during different periods in the haul to see if we can pinpoint the source of noise.\r\nPropagation modeling\r\nLink to the longline-propagation repository which provides an overview of a few of the Phase III data sets and links to a ‘Status Report’ containing preliminary results.\r\nBack to top\r\nLast Updated\r\n 15 March 2023\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-14T21:01:38-07:00"
    },
    {
      "path": "vessels.html",
      "title": "Vessel Facilitation",
      "description": "Info, protocols, and links relevant for vessel facilitation.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nBackground\r\nTrip initiation protocol\r\nDatasheets and instructions for vessels\r\nInstructional documents\r\nDatasheets\r\nHow to download\r\n\r\nFliers and outreach materials\r\nVessel list\r\nLast Updated\r\n\r\n\r\n\r\n\r\n\r\nBackground\r\nVessel facilitation is a key component in ensuring the Longline Acoustic Monitoring Project is successful. We rely on volunteer vessels to take our equipment out with them and record a small amount of relevant metadata.\r\nWhen one of these volunteer vessels is assigned a fisheries observer (by the random lottery system), then we would like to send our acoustic equipment out with them. We prefer to send gear when an observer is on board because the observer collects additional data on the catch and marine mammal depredation that may be useful for our analyses.\r\nAdditionally, our gear team needs at least 48 hours to prepare the acoustic gear for deployment. This process includes programming, testing, and installing new batteries, and can be completed in a day if all three people capable of doing this are available. We are not able to do this weeks in advance because as soon as the gear is in “standby” mode, there is significant power draw on the batteries. The more planning time we have, the better chance of getting a successful trip out the door.\r\nVessel facilitation includes:outreach - connecting with vessels and getting them on board to participate in the projectcoordination - communication with a vessel when it is selected for an observer to confirm participation and arrange a time for gear hand off and pick upequipment transfer - delivery our acoustic gear from Ford Island to the vessel at the docks, and return of equipment after a tripinstruction - a meeting with the captain, crew, and observer to hand off blank datasheets and go over what is expected of them to collect good datadebrief - picking up filled out datasheets and collecting any feedback from the crew and observer about successes or failures during the trip that we can improve on, and delivering the report of our findings to the vessel\r\nBack to top\r\nTrip initiation protocol\r\nThis is the basic “order of operations” necessary to make a trip happen. Points of communication are in bold italics and specific personnel are bold.\r\nSTEP 1 - Check equipment availability.\r\nAt the start of each week, S check in with E (and/or our others in CRP who can prep gear if E is out) via phone or GChat or email to get their availability for prepping the acoustic gear that week.\r\nSTEP 2 - Get a vessel.S lets J at the Observer Program know we are a “go” for the coming week. The Observer Program has our list of confirmed vessels and will check any vessels that get assigned an observer against that list. If one of our vessels is assigned an experienced observer, J will email S with information on the vessel, observer, when the vessel plans to depart, and will cc the port coordinator for the week.\r\nSTEP 3 - Confirm equipment prep/delivery is possible.S informs E and N via phone or GChat that one of our vessels is planning to depart and confirms that the planned departure date and time is workable for all. N calls the captain or owner to confirm they are willing to participate. N updates S via phone that the vessel is on board. S then confirms with E (and/or prep team) via phone or Gchat that the deployment is a GO and gets an estimate of when the gear will be ready by.\r\nSTEP 4 - Coordinate equipment delivery.\r\nS lets J and the port coordinator know via email or phone that we are all set to send gear with this vessel and gets the final observer placement time. Depending on the flexibility of the departure time, S may connect the port coordinator and N so they can compare schedules and set up an observer placement time that works for the vessel and our research team.\r\nSTEP 5 - Prep equipment.E and team prep the recorders. Team texts S and N know the gear is ready and can be picked up from Ford Island. For the first trip with N picking up gear, E will meet him for the handoff and answer any questions about the actual recorders during the handoff. N texts S to confirm that gear has been picked up.\r\nSTEP 6 - Equipment delivery to vessel.N delivers gear to vessel and goes over the instruction and datasheets with the crew, captain, and observer all at once. If any questions come up during the deliver, N calls S to answer any questions. N texts S when gear has been successfully delivered!\r\n Vessel departs and fishes  and records sounds \r\nSTEP 7 - Equipment pickup from vessel and return to lab.\r\nThe observer will contact N when the vessel returns to port. N arranges a time to pick up the equipment and datasheets from the vessel, either from the observer or from the crew. N informs S via email or phone that the vessel is back in port and when the gear will be picked up from the vessel and dropped off at Ford Island. S tells E via Gchat or email or phone when the gear will be back at the lab. E tells S via Gchat or email or phone when the gear is in the lab and when the planned data offload will begin.\r\nBack to top\r\nDatasheets and instructions for vessels\r\nCopies of all of current versions for the below documents can also be found in the Blank Data Sheets and Flyers_migratedToGitHub folder on Google Drive\r\nInstructional documents\r\nFor our current Phase 3 of this project, with multiple recorders and in-air recordings, the instruction document is:\r\nInstructionsForVesselAndObserver.pdf\r\nDrafts and old instruction documents for single-recorder trips can be found in instructions for vessels GitHub folder\r\nDatasheets\r\nGitHub folder with all blank datasheets\r\nDatasheets for underwater recorders. A new datasheet is needed for each set, and up to 8 sets are monitored per trip, so at least 8 datasheets should be provided for each trip. We are currently running Multiple Recoder Trips.\r\nObserver Datasheet - Multiple Recorder TripObserver Datasheet - Single Recorder Trip\r\nDatasheet for in-air recorders. One datasheet is needed per trip; one datasheet has room for data for up to 8 sets. The datasheet also includes detailed recorder operation instructions.\r\nObserver Datasheet - In Air Recorders\r\nHow to download\r\nFollow the above link to the datasheet of interest. If you access the folder of all datasheets, just click on the datasheet you want to download to be taken to it’s individual page\r\nClick the “Download” button on the upper-middle/right of the page\r\nTo download pdf from the GitHub folder, navigate to the document you want and use the Download button on the right hand side.Back to top\r\nFliers and outreach materials\r\nWe have a few informal outreach materials:\r\nAn informational flier that can be distributed to potential interested vessels. There is an English and Vietnamese version.\r\nA PDF of two Powerpoint slides providing a general description of the project and the sound of interest.\r\nThese can be found in the fliers_outreach GitHub Folder\r\nBack to top\r\nVessel list\r\nConfirmed Vessel ListAccess to this document is restricted to just those within the project since it contains contact information for certain vessel captains and owners as well as trip timing information\r\nBack to top\r\nLast Updated\r\n 15 March 2023\r\n\r\n\r\n\r\n",
      "last_modified": "2023-03-14T21:01:38-07:00"
    }
  ],
  "collections": []
}
